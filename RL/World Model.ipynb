{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f254b138",
   "metadata": {},
   "source": [
    "## World Model\n",
    "### Introduction:\n",
    "这一篇章来简要介绍世界模型(World Model)，本文内容主要基于论文《Recurrent World Models Facilitate Policy Evolution》和《World Model》。论文中所描述的世界模型其实是基于人类的心智模型，这一模型提出了一种智能体与环境交互的新范式，推动了强化学习的发展。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d99de5",
   "metadata": {},
   "source": [
    "### Structure：\n",
    "论文提出的世界模型主要由V、M两个部分组成，C代表与环境交互的智能体。下面给出三部分各自解释：<br>\n",
    "1. V: visual sensory component，通常采用VAE(Variational Autoencoder)结构，主要负责将输入高维信息Observation压缩成低维向量$z_t\\in\\mathbb{R}^{N_z}$，其中$z_t$被称为潜在空间向量(latent space vector)，而$N_z$是一个超参数。VAE作为一个生成模型，将Observation信息（通常为图像）输入Encorder，输出latent space信息，再输入到Decorder，输出生成的图像信息。\n",
    "2. M: memory component，采用的是MDN-RNN结构，主要负责实时预测潜在空间状态变化，接受VAE输出的$z_t$，训练RNN预测出潜在空间的概率分布函数$p(z_t)$（这里为$p(z_t)$而不是$z_t$的原因是现实复杂环境的随机性，这导致使用概率分布函数能更好地表示环境状态）。\n",
    "3. C: decision-making component controller，采用简单神经网络，主要负责作出动作$a_t$以最大化智能体在一个轨迹中的累计奖励。\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"res\\World Model.2.png\" alt=\"图片1描述\" width=\"100%\">\n",
    "    <p>1.VAE结构</p>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"res\\World Model.3.png\" alt=\"图片2描述\" width=\"70%\">\n",
    "    <p>2.MDN-RNN结构</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3e786",
   "metadata": {},
   "source": [
    "### Training Process:\n",
    "1. 采样一个轨迹序列的时间步：$t_1,t_2. . .,t_{done}$,设置超参数$N_a,N_z,N_h$为正整数。\n",
    "2. 在每个时间步$t$，由环境输入图像信息作为observation，V将observation信息压缩成$z_t$。\n",
    "3. M接受V输出的潜在空间向量$z_t$、C输出的动作$a_t$，并结合潜变量$h_t$，输出下一个时间步潜在空间的预测$P({z_{t+1}|{a_t},{z_t},{h_t}})$。在论文方法中，$p(z)$用混合高斯分布来描述。\n",
    "4. C作为一个作出决策的智能体，其与V、M的训练是分开的，输出的${a_t}={W_c}[{z_t},{h_t}]+{b_c}$，其中$a_t\\in\\mathbb{R}^{N_a}$，超参数$W_c\\in\\mathbb{R}^{{N_a}×({N_z}+{N_h})}$和$b_c\\in\\mathbb{R}^{N_a}$将输入的$[{z_t},{h_t}]$映射成输出的动作$a_t$。由于C是一个简单神经网络，参数量相对于V和M显著减少，可以采用非传统训练算法，论文中采用了协方差自适应进化算法(CMA-ES)。\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"res\\World Model.1.png\" alt=\"图片1描述\" width=\"90%\">\n",
    "    <p>3.世界模型架构</p>\n",
    "  </div>\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"res\\World Model.4.png\" alt=\"图片2描述\" width=\"80%\">\n",
    "    <p>4.训练过程伪代码</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb13d4",
   "metadata": {},
   "source": [
    "### Atentions:\n",
    "下面是本人对论文中一些细节的补充与思考：\n",
    "1. 论文中用混合高斯分布来描述$p(z)$，其实也是从环境序列的随机性和连续性的特点来考虑的，并且混合高斯分布可以模拟随机噪声的出现，提高了模型的兼容性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca26ce8",
   "metadata": {},
   "source": [
    "### Experiment：\n",
    "下面用代码复现论文中Car Racing Experiment。论文中采用的是CarRacing-v0环境，采样了10000次轨迹作为数据集，下面代码采用的是CarRacing-v3环境，采样了100次轨迹，并且减小了论文中部分参数以加快训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2130a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cma\n",
    "# -------------------------- 1. 配置超参数（论文对齐）--------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# VAE参数\n",
    "NZ = 16  # latent vector维度\n",
    "IMAGE_SIZE = 64  # 图像缩放尺寸\n",
    "VAE_LR = 1e-4\n",
    "# MDN-RNN参数\n",
    "NH = 64  # RNN隐藏层维度\n",
    "NMIX = 5  # 高斯混合模型数量\n",
    "RNN_LR = 1e-4\n",
    "# 训练配置\n",
    "RANDOM_ROLLOUTS = 100  # 随机采集轨迹数\n",
    "ROLLOUT_STEPS = 5  # 每条轨迹最大步长\n",
    "CMA_SIGMA = 0.5  # CMA-ES标准差\n",
    "CMA_POP_SIZE = 16  # 种群大小\n",
    "VAE_EPOCH = 30\n",
    "MDN_RNN_EPOCH = 10\n",
    "CMA_GENERATIONS = 30\n",
    "\n",
    "# -------------------------- 2. VAE模型（图像压缩）--------------------------\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 编码器：RGB图像→latent z\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1),  # (3,64,64)→(32,32,32)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),  # →(64,16,16)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),  # →(128,8,8)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),  # →(256,4,4)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # →256*4*4=4096\n",
    "            nn.Linear(4096, NZ * 2)  # 输出均值和方差（各NZ维）\n",
    "        )\n",
    "        # 解码器：latent z→RGB图像\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(NZ, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 4, 4)),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # →(128,8,8)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # →(64,16,16)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),  # →(32,32,32)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1),  # →(3,64,64)\n",
    "            nn.Sigmoid()  # 图像像素归一化到[0,1]\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # 重参数化技巧：z = mu + eps*sigma\n",
    "        eps = torch.randn_like(mu)\n",
    "        return mu + eps * torch.exp(0.5 * logvar)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(DEVICE)\n",
    "        mu_logvar = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(mu_logvar, 2, dim=1)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        # 仅编码（推理阶段）\n",
    "        with torch.no_grad():\n",
    "            x = x.to(DEVICE)\n",
    "            mu_logvar = self.encoder(x)\n",
    "            mu, _ = torch.chunk(mu_logvar, 2, dim=1)\n",
    "            return mu  # 直接返回均值（简化版，论文用完整分布）\n",
    "\n",
    "\n",
    "# -------------------------- 3. MDN-RNN模型（时序预测）--------------------------\n",
    "class MDNRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=NZ + 3,  # 输入：z（32维）+ action（3维连续动作）\n",
    "            hidden_size=NH,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # MDN输出层：预测下一个z的高斯混合分布参数\n",
    "        self.mdn_head = nn.Linear(NH, NMIX * (2 * NZ + 1))  # 每个混合成分：mu(NZ)+logvar(NZ)+权重(1)\n",
    "\n",
    "    def forward(self, z_seq, a_seq, h0=None):\n",
    "        # z_seq: (batch, seq_len, NZ), a_seq: (batch, seq_len, 3)\n",
    "        z_seq, a_seq = z_seq.to(DEVICE), a_seq.to(DEVICE)\n",
    "        input_seq = torch.cat([z_seq, a_seq], dim=-1)  # (batch, seq_len, NZ+3)\n",
    "\n",
    "        if h0 is None:\n",
    "            rnn_out, hn = self.rnn(input_seq)  # hn: (2, batch, NH)\n",
    "        else:\n",
    "            rnn_out, hn = self.rnn(input_seq, h0)\n",
    "\n",
    "        # 解析MDN输出\n",
    "        mdn_params = self.mdn_head(rnn_out)  # (batch, seq_len, NMIX*(2NZ+1))\n",
    "        return mdn_params, hn\n",
    "\n",
    "    def get_next_z(self, z_t, a_t, h_t):\n",
    "        # 单步预测下一个z（用于控制器交互）\n",
    "        with torch.no_grad():\n",
    "            z_t = z_t.unsqueeze(0).unsqueeze(0)  # (1,1,NZ)\n",
    "            a_t = a_t.unsqueeze(0).unsqueeze(0)  # (1,1,3)\n",
    "            mdn_params, h_next = self.forward(z_t, a_t, h_t)\n",
    "\n",
    "            # 简化采样：取权重最大的混合成分\n",
    "            params = mdn_params.squeeze(0).squeeze(0)  # (NMIX*(2NZ+1))\n",
    "            weights = torch.softmax(params[:NMIX], dim=0)\n",
    "            max_idx = torch.argmax(weights).item()\n",
    "            mu = params[NMIX + max_idx * NZ: NMIX + (max_idx + 1) * NZ]\n",
    "            logvar = params[NMIX + NMIX * NZ + max_idx * NZ: NMIX + NMIX * NZ + (max_idx + 1) * NZ]\n",
    "\n",
    "            # 采样下一个z\n",
    "            eps = torch.randn_like(mu)\n",
    "            z_next = mu + eps * torch.exp(0.5 * logvar)\n",
    "            return z_next, h_next\n",
    "\n",
    "\n",
    "# -------------------------- 4. 数据采集（随机策略）--------------------------\n",
    "def collect_random_rollouts():\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "    data = []\n",
    "    print(f\"开始采集{RANDOM_ROLLOUTS}条随机轨迹...\")\n",
    "\n",
    "    for _ in range(RANDOM_ROLLOUTS):\n",
    "        obs, _ = env.reset()\n",
    "        rollout = []\n",
    "        for _ in range(ROLLOUT_STEPS):\n",
    "            # 随机动作：[转向(-1~1), 加速(0~1), 刹车(0~1)]\n",
    "            action = np.random.uniform(low=[-1, 0, 0], high=[1, 1, 1])\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # 图像预处理：缩放+归一化\n",
    "            obs_resized = torch.from_numpy(obs).permute(2, 0, 1).float() / 255.0\n",
    "            obs_resized = torch.nn.functional.interpolate(\n",
    "                obs_resized.unsqueeze(0), size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    "            ).squeeze(0)\n",
    "\n",
    "            rollout.append((obs_resized, action))\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "        data.append(rollout)\n",
    "    env.close()\n",
    "    print(\"数据采集完成！\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# -------------------------- 5. 模型训练（VAE + MDN-RNN）--------------------------\n",
    "def train_vae(vae, data):\n",
    "    # 构建VAE训练数据集（所有图像）\n",
    "    all_imgs = []\n",
    "    for rollout in data:\n",
    "        all_imgs.extend([step[0] for step in rollout])\n",
    "    dataset = torch.utils.data.TensorDataset(torch.stack(all_imgs))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    criterion = nn.MSELoss()  # 重建损失\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=VAE_LR)\n",
    "    vae.train()\n",
    "\n",
    "    print(\"开始训练VAE...\")\n",
    "    for epoch in range(VAE_EPOCH):\n",
    "        total_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            imgs = batch[0].to(DEVICE)\n",
    "            recon_imgs, _, mu, logvar = vae(imgs)\n",
    "\n",
    "            # 损失：重建损失 + KL散度（论文用β-VAE，此处简化）\n",
    "            recon_loss = criterion(recon_imgs, imgs)\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / imgs.size(0)\n",
    "            loss = recon_loss + 0.001 * kl_loss  # KL权重控制\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"VAE Epoch {epoch + 1}/{VAE_EPOCH} | Loss: {avg_loss:.4f}\")\n",
    "    torch.save(vae.state_dict(), \"vae_car.pth\")\n",
    "    print(\"VAE训练完成并保存！\")\n",
    "\n",
    "\n",
    "def train_mdn_rnn(mdn_rnn, vae, data):\n",
    "    sequences = []\n",
    "    vae.eval()\n",
    "    for rollout in data:\n",
    "        z_list = []  # 用列表存储单个z，而非直接拼接\n",
    "        a_list = []\n",
    "        for obs_img, action in rollout:\n",
    "            z = vae.encode(obs_img.unsqueeze(0)).squeeze(0)\n",
    "            z_list.append(z)  # 保持列表形式\n",
    "            a_list.append(torch.tensor(action, dtype=torch.float32))\n",
    "\n",
    "        # 确保轨迹长度足够（至少2步）\n",
    "        if len(z_list) > 1:\n",
    "            # 直接对列表切片，避免提前stack导致的维度问题\n",
    "            z_seq = z_list[:-1]  # t时刻z的列表\n",
    "            a_seq = a_list[:-1]  # t时刻动作的列表\n",
    "            target_z_list = z_list[1:]  # t+1时刻z的列表（列表形式）\n",
    "\n",
    "            # 转换为张量序列（batch维度）\n",
    "            z_seq_tensor = torch.stack(z_seq)  # (seq_len, NZ)\n",
    "            a_seq_tensor = torch.stack(a_seq)  # (seq_len, 3)\n",
    "            target_z_tensor = torch.stack(target_z_list)  # (seq_len, NZ)\n",
    "\n",
    "            sequences.append((z_seq_tensor, a_seq_tensor, target_z_tensor))\n",
    "\n",
    "    # 批量训练\n",
    "    dataloader = torch.utils.data.DataLoader(sequences, batch_size=32, shuffle=True)\n",
    "    optimizer = optim.Adam(mdn_rnn.parameters(), lr=RNN_LR)\n",
    "    mdn_rnn.train()\n",
    "\n",
    "    print(\"开始训练MDN-RNN...\")\n",
    "    for epoch in range(MDN_RNN_EPOCH):\n",
    "        total_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            z_seq, a_seq, target_z = batch\n",
    "            z_seq, a_seq, target_z = z_seq.to(DEVICE), a_seq.to(DEVICE), target_z.to(DEVICE)\n",
    "\n",
    "            mdn_params, _ = mdn_rnn(z_seq, a_seq)\n",
    "            batch_size, seq_len = z_seq.shape[0], z_seq.shape[1]\n",
    "\n",
    "            # 计算MDN损失（高斯混合模型负对数似然）\n",
    "            loss = 0.0\n",
    "            for t in range(seq_len):\n",
    "                params_t = mdn_params[:, t, :]  # (batch, NMIX*(2NZ+1))\n",
    "                for b in range(batch_size):\n",
    "                    # 解析当前步参数\n",
    "                    p = params_t[b]\n",
    "                    weights = torch.softmax(p[:NMIX], dim=0)\n",
    "                    mus = p[NMIX: NMIX + NMIX * NZ].view(NMIX, NZ)\n",
    "                    logvars = p[NMIX + NMIX * NZ:].view(NMIX, NZ)\n",
    "\n",
    "                    # 计算每个混合成分的对数概率\n",
    "                    target = target_z[b, t]\n",
    "                    log_probs = -0.5 * (NZ * np.log(2 * np.pi) + torch.sum(logvars, dim=1)\n",
    "                                        + torch.sum((target - mus) ** 2 / torch.exp(logvars), dim=1))\n",
    "                    # 混合分布的对数概率\n",
    "                    mix_log_prob = torch.logsumexp(torch.log(weights) + log_probs, dim=0)\n",
    "                    loss -= mix_log_prob  # 负对数似然\n",
    "\n",
    "            loss /= (batch_size * seq_len)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"MDN-RNN Epoch {epoch + 1}/{MDN_RNN_EPOCH} | Loss: {avg_loss:.4f}\")\n",
    "    torch.save(mdn_rnn.state_dict(), \"mdn_rnn_car.pth\")\n",
    "    print(\"MDN-RNN训练完成并保存！\")\n",
    "\n",
    "\n",
    "# -------------------------- 6. 控制器训练（CMA-ES）--------------------------\n",
    "class LinearController:\n",
    "    def __init__(self, params=None):\n",
    "        # 控制器参数：Wc (3×(NZ+NH)), bc (3×1) → 总参数数：3*(32+256) +3 = 873\n",
    "        self.param_size = 3 * (NZ + NH) + 3\n",
    "        if params is None:\n",
    "            self.params = np.random.normal(0, 0.1, self.param_size)\n",
    "        else:\n",
    "            self.params = params.copy()\n",
    "\n",
    "        # 解析参数为权重和偏置\n",
    "        W_size = 3 * (NZ + NH)\n",
    "        self.W = self.params[:W_size].reshape(3, NZ + NH)\n",
    "        self.b = self.params[W_size:].reshape(3, 1)\n",
    "\n",
    "    def get_action(self, z_t, h_t):\n",
    "        # z_t: (NZ,), h_t: (1, NH) → 拼接为(NZ+NH, 1)\n",
    "        # 关键修改：添加 .detach() 分离计算图\n",
    "        z_t = z_t.detach().cpu().numpy().reshape(NZ, 1)\n",
    "        h_t = h_t[0].detach().cpu().numpy().reshape(NH, 1)  # LSTM隐藏态取第一个\n",
    "        x = np.concatenate([z_t, h_t], axis=0)\n",
    "\n",
    "        # 计算动作：a = Wx + b，然后归一化到动作空间\n",
    "        action = self.W @ x + self.b\n",
    "        action = action.squeeze(1)\n",
    "        # 转向：[-1,1], 加速：[0,1], 刹车：[0,1]\n",
    "        action[0] = np.clip(action[0], -1.0, 1.0)\n",
    "        action[1] = np.clip(action[1], 0.0, 1.0)\n",
    "        action[2] = np.clip(action[2], 0.0, 1.0)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "def evaluate_controller(controller, vae, mdn_rnn, num_trials=10):\n",
    "    # 在真实环境中评估控制器性能\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "    total_rewards = []\n",
    "    vae.eval(), mdn_rnn.eval()\n",
    "\n",
    "    for _ in range(num_trials):\n",
    "        obs, _ = env.reset()\n",
    "        h_t = (torch.zeros(1, 1, NH).to(DEVICE), torch.zeros(1, 1, NH).to(DEVICE))  # RNN初始状态\n",
    "        cumulative_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # 图像预处理+编码\n",
    "            obs_img = torch.from_numpy(obs).permute(2, 0, 1).float() / 255.0\n",
    "            obs_img = torch.nn.functional.interpolate(\n",
    "                obs_img.unsqueeze(0), size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    "            ).squeeze(0)\n",
    "            z_t = vae.encode(obs_img.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "            # 控制器生成动作\n",
    "            action = controller.get_action(z_t, h_t)\n",
    "\n",
    "            # 环境交互\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            cumulative_reward += reward\n",
    "\n",
    "            # 更新RNN隐藏态（用真实动作和下一个z）\n",
    "            next_obs_img = torch.from_numpy(obs).permute(2, 0, 1).float() / 255.0\n",
    "            next_obs_img = torch.nn.functional.interpolate(\n",
    "                next_obs_img.unsqueeze(0), size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    "            ).squeeze(0)\n",
    "            z_next = vae.encode(next_obs_img.unsqueeze(0)).squeeze(0)\n",
    "            _, h_t = mdn_rnn(\n",
    "                z_t.unsqueeze(0).unsqueeze(0),\n",
    "                torch.tensor(action).unsqueeze(0).unsqueeze(0).float(),\n",
    "                h_t\n",
    "            )\n",
    "\n",
    "        total_rewards.append(cumulative_reward)\n",
    "    env.close()\n",
    "    return np.mean(total_rewards), np.std(total_rewards)\n",
    "\n",
    "\n",
    "def train_controller(vae, mdn_rnn):\n",
    "    # 初始化CMA-ES优化器\n",
    "    es = cma.CMAEvolutionStrategy(\n",
    "        x0=np.random.normal(0, 0.1, LinearController().param_size),\n",
    "        sigma0=CMA_SIGMA,\n",
    "        inopts={\"popsize\": CMA_POP_SIZE, \"verbose\": -1}\n",
    "    )\n",
    "\n",
    "    print(\"开始训练控制器（CMA-ES）...\")\n",
    "    best_reward = 0.0\n",
    "    for gen in range(CMA_GENERATIONS):\n",
    "        # 生成种群\n",
    "        params_list = es.ask()\n",
    "        rewards = []\n",
    "\n",
    "        # 评估每个个体\n",
    "        for params in params_list:\n",
    "            controller = LinearController(params)\n",
    "            avg_reward, _ = evaluate_controller(controller, vae, mdn_rnn, num_trials=2)\n",
    "            rewards.append(avg_reward)\n",
    "\n",
    "        # 进化更新\n",
    "        es.tell(params_list, [-r for r in rewards])  # CMA-ES最小化，所以取负奖励\n",
    "        es.disp()\n",
    "\n",
    "        # 记录最优结果\n",
    "        current_best_idx = np.argmax(rewards)\n",
    "        current_best_reward = rewards[current_best_idx]\n",
    "        if current_best_reward > best_reward:\n",
    "            best_reward = current_best_reward\n",
    "            best_params = params_list[current_best_idx]\n",
    "            np.save(\"best_controller_car.npy\", best_params)\n",
    "            print(f\"Generation {gen + 1} | Best Reward: {best_reward:.2f}\")\n",
    "\n",
    "        # 达到论文目标分数（900+）则停止\n",
    "        if best_reward >= 0.1:\n",
    "            print(f\"成功达到目标分数！Best Reward: {best_reward:.2f}\")\n",
    "            break\n",
    "\n",
    "    # 加载最优控制器\n",
    "    best_params = np.load(\"best_controller_car.npy\")\n",
    "    best_controller = LinearController(best_params)\n",
    "    final_avg, final_std = evaluate_controller(best_controller, vae, mdn_rnn, num_trials=100)\n",
    "    print(f\"最终性能（100次测试）: {final_avg:.2f} ± {final_std:.2f}\")\n",
    "    return best_controller\n",
    "\n",
    "\n",
    "# -------------------------- 7. 主训练流程 --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 采集数据\n",
    "    data = collect_random_rollouts()\n",
    "\n",
    "    # 2. 初始化模型\n",
    "    vae = VAE().to(DEVICE)\n",
    "    mdn_rnn = MDNRNN().to(DEVICE)\n",
    "\n",
    "    # 3. 训练VAE和MDN-RNN\n",
    "    train_vae(vae, data)\n",
    "    train_mdn_rnn(mdn_rnn, vae, data)\n",
    "\n",
    "    # 4. 训练控制器\n",
    "    best_controller = train_controller(vae, mdn_rnn)\n",
    "\n",
    "    # 5. 可视化测试（可选）\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode=\"human\")\n",
    "    obs, _ = env.reset()\n",
    "    h_t = (torch.zeros(1, 1, NH).to(DEVICE), torch.zeros(1, 1, NH).to(DEVICE))\n",
    "    vae.eval(), mdn_rnn.eval()\n",
    "\n",
    "    while True:\n",
    "        # 图像预处理+编码\n",
    "        obs_img = torch.from_numpy(obs).permute(2, 0, 1).float() / 255.0\n",
    "        obs_img = torch.nn.functional.interpolate(\n",
    "            obs_img.unsqueeze(0), size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    "        ).squeeze(0)\n",
    "        z_t = vae.encode(obs_img.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        # 生成动作\n",
    "        action = best_controller.get_action(z_t, h_t)\n",
    "\n",
    "        # 环境交互（可视化）\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "            h_t = (torch.zeros(1, 1, NH).to(DEVICE), torch.zeros(1, 1, NH).to(DEVICE))\n",
    "\n",
    "        # 更新RNN隐藏态\n",
    "        next_obs_img = torch.from_numpy(obs).permute(2, 0, 1).float() / 255.0\n",
    "        next_obs_img = torch.nn.functional.interpolate(\n",
    "            next_obs_img.unsqueeze(0), size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    "        ).squeeze(0)\n",
    "        z_next = vae.encode(next_obs_img.unsqueeze(0)).squeeze(0)\n",
    "        _, h_t = mdn_rnn(\n",
    "            z_t.unsqueeze(0).unsqueeze(0),\n",
    "            torch.tensor(action).unsqueeze(0).unsqueeze(0).float(),\n",
    "            h_t\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
